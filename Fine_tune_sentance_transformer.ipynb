{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cc7e6c-a547-4032-af67-aeccc4963b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31e0ad8-5b2d-4147-9ee1-9aa71a901115",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qac=pd.read_csv('all_qac_triplets.csv').drop_duplicates(subset=['original_ques','context','filename'])[['original_ques','context','filename']]\n",
    "#all_qac=pd.read_csv('all_qac_triplets.csv')\n",
    "all_qac.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c56319-f429-4b63-b624-4b2b954cf1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = all_qac.drop_duplicates(subset=['context'])\n",
    "unique_context = contexts[\"context\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5b0d7-811a-4a69-961e-0657c74153e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_context[0:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fb1376-ba5e-4333-b0f0-07f87e11ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def break_into_sentences(passage):\n",
    "    sentences = nltk.sent_tokenize(passage)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57102500-823e-407c-8137-4d3388fa1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "break_into_sentences(unique_context[3363])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b00b21-5edf-4914-86e1-09670b89dc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e2ca3-3fab-417a-ae20-6212cb5fe0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from textblob import TextBlob\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c78e3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_least_two_sensible_sentences(context):\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    english_sentences = [sentence for sentence in sentences if is_english(sentence)]\n",
    "    return len(english_sentences) >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9635a4-359f-4f02-bd18-b52ee1ea6980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting random sentances from contexts\n",
    "# import random\n",
    "# def get_english_contexts(unique_context, target_count=1000):\n",
    "#     context_length = len(unique_context)\n",
    "#     selected_context = []\n",
    "#     random_numbers=[]\n",
    "#     while len(selected_context) < target_count:\n",
    "#         random_index = random.randint(0, context_length - 1)\n",
    "#         #print(at_least_two_sensible_sentences(unique_context[random_index]))\n",
    "#         if at_least_two_sensible_sentences(unique_context[random_index]): # type: ignore\n",
    "#             if unique_context[random_index] not in selected_context:\n",
    "#                 selected_context.append(unique_context[random_index])\n",
    "#                 random_numbers.append(random_index)\n",
    "\n",
    "#     return selected_context,random_numbers\n",
    "\n",
    "# selected_context,random_numbers = get_english_contexts(unique_context)\n",
    "\n",
    "# # Display the first two selected contexts\n",
    "# selected_context[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1083bb12-af45-4dc9-b6b7-a006a4d02cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open('selected_context.pkl','wb') as f:\n",
    "#     pickle.dump(random_numbers,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad246b-68f0-420f-ad3a-8a0779f25a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('selected_context.pkl','rb') as f:\n",
    "    random_numbers=pickle.load(f)\n",
    "#random_numbers    \n",
    "selected_context = [unique_context[i] for i in random_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55936654-c14f-4b11-831f-8fd6d8cfcbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_english_and_sensible(sentence):\n",
    "    try:\n",
    "        # Detect language\n",
    "        if detect(sentence) != 'en':\n",
    "            return False\n",
    "        \n",
    "        # Use TextBlob to check if the sentence makes sense (basic check)\n",
    "        blob = TextBlob(sentence)\n",
    "        if len(blob.words) < 3 or not blob.correct():\n",
    "            return False\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c069334-d8b4-40e2-8ea3-7debacf7a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select sentances \n",
    "# import random\n",
    "# sentance_number={}\n",
    "# sentances=[]\n",
    "# for i in range(len(selected_context)):\n",
    "#     if(i%100==0):\n",
    "#      print(i)\n",
    "#     context=selected_context[i]\n",
    "#     sentances=break_into_sentences(context)\n",
    "#     random_num=0\n",
    "#     is_valid=False\n",
    "#     while(not is_valid):\n",
    "#         random_num = random.randrange(0, len(sentances))\n",
    "#         sentence=sentances[random_num]\n",
    "#         is_valid=is_english_and_sensible(sentence)\n",
    "#         #print(is_valid)\n",
    "#     sentance_number[i]=random_num\n",
    "#     sentances.append(sentence)\n",
    "    \n",
    "# sentances    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e83670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# with open('selected_sentence.json','w') as f:\n",
    "#     json.dump(sentance_number,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('selected_sentence.json','r') as f:\n",
    "    sentance_number=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a935bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentance_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9731bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_sentances=[]\n",
    "for i in range(len(selected_context)):\n",
    "    if(i%100==0):\n",
    "     print(i)\n",
    "    context=selected_context[i]\n",
    "    sentance=break_into_sentences(context)[sentance_number[str(i)]]\n",
    "    \n",
    "    \n",
    "    selected_sentances.append(sentance)\n",
    "    \n",
    "selected_sentances   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a7a690-4e8e-4ab1-b6d6-ea5348af888c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(selected_sentances)) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa18bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt to rephrase the text\n",
    "def prompt1(alltext):\n",
    "    return f'''You will be provided with text snippets extracted from a clinical trial protocol document. Your task is to generate up to two rephrased versions for each text snippet. Below are the text snippets:\n",
    "\n",
    "<text>\n",
    "\n",
    "{alltext}\n",
    "\n",
    "</text>\n",
    "\n",
    "Please generate the rephrased versions of the text snippets. Only rephrase the text given within the tags.\n",
    "\n",
    "Each original text snippet and its rephrased versions should follow this format:\n",
    "T1: Original text.\n",
    "T1.1: First rephrased version.\n",
    "T1.2: Second rephrased version.\n",
    "\n",
    "To create effective paraphrases, avoid using the same words as in the original text. For example, if the original text is 'What is the patient's age?' a good paraphrase could be 'How old is the patient?' or 'What is the age of the patient?'.\n",
    "\n",
    "The overall format of your response should match what is shown between the tags. Ensure to strictly follow the formatting and spacing.\n",
    "\n",
    "<example>\n",
    "\n",
    "T1: Patients were assessed for adverse events.\n",
    "T1.1: Patients were evaluated for side effects.\n",
    "T1.2: Adverse events in patients were monitored.\n",
    "\n",
    "T2: The primary endpoint was overall survival.\n",
    "T2.1: The main outcome measure was overall survival.\n",
    "T2.2: Overall survival was the primary endpoint.\n",
    "\n",
    "T3: Informed consent was obtained from all participants.\n",
    "T3.1: All participants provided informed consent.\n",
    "T3.2: Consent was obtained from all participants in an informed manner.\n",
    "\n",
    "</example>\n",
    "\n",
    "Please provide up to two rephrased versions for each text snippet. Ensure that the output includes only the original text and their rephrased versions, formatted as shown in the example. The rephrased text should maintain the same meaning as the original text. Make sure the original text snippet is present in the output and follows the specified format.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad53c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt(prompt):\n",
    "    #https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "    return f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant. Your carefully follow instructions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cf2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "j=0\n",
    "prompts=[]\n",
    "while(j!=len(selected_sentances)):\n",
    "    text=''\n",
    "    for i in range(10):\n",
    "        modified_sentence = selected_sentances[i+j].replace('\\n', ' ')\n",
    "        text=text+f\"Q{i+1}:{modified_sentence}\\n\"\n",
    "    prompt=system_prompt(prompt1(text))\n",
    "    prompts.append(prompt) \n",
    "    j=j+10  \n",
    "print(len(prompts))      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "llm = LLM(model=\"/lockbox/models/Meta-Llama-3-8B-Instruct\", gpu_memory_utilization = 0.8,enforce_eager=True)\n",
    "sampling_params = SamplingParams(temperature=0.05, top_p=0.95, top_k = 40, max_tokens = 4816,)\n",
    "out=[]\n",
    "try:\n",
    "    generated_outputs = llm.generate(prompts, sampling_params)\n",
    "    for output in generated_outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        out.append(generated_text) \n",
    "except Exception:\n",
    "            print(Exception)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c470842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(out)):\n",
    "#     temp=out[i]\n",
    "#     filename=f'./Train_data/all_rephrased_text/out{i}.txt'\n",
    "#     with open(filename,'w') as f:\n",
    "#         f.write(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_rephrased_questions(file_path):\n",
    "    # Read the text file\n",
    "    with open(file_path, 'r') as file:\n",
    "        file_content = file.read()\n",
    "\n",
    "    # Define the pattern to match the rephrased questions\n",
    "    pattern = r'^Q\\d+\\.\\d+: (.+)$'\n",
    "\n",
    "\n",
    "    # Find all matches using re.findall\n",
    "    rephrased_questions = re.findall(pattern, file_content, re.MULTILINE)\n",
    "\n",
    "    return rephrased_questions\n",
    "\n",
    "rephrased_questions = extract_rephrased_questions('./Train_data/all_rephrased_text/out98.txt')\n",
    "rephrased_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef578bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "path='./Train_data/all_rephrased_text/'\n",
    "rephrased=[]\n",
    "for j in range(100):\n",
    "    file=f\"out{j}.txt\"\n",
    "    full_path=path+file\n",
    "    rephrased_questions = extract_rephrased_questions(full_path)\n",
    "    if(len(rephrased_questions)!=20):\n",
    "        print(full_path)\n",
    "    for i in range(0,len(rephrased_questions),2):\n",
    "            num=random.randint(0, 1)\n",
    "            #print(num)\n",
    "            if(num==0):rephrased.append(rephrased_questions[i])\n",
    "            elif(num==1):rephrased.append(rephrased_questions[i+1])\n",
    "    \n",
    "\n",
    "print(len(rephrased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832e30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def at_least_one_sensible_sentences(context):\n",
    "    sentences = nltk.sent_tokenize(context)\n",
    "    english_sentences = [sentence for sentence in sentences if is_english(sentence)]\n",
    "    return len(english_sentences) >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5636813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a random context from the same or a different context\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "seen = set()\n",
    "negatives = []\n",
    "\n",
    "for i, context in tqdm(enumerate(selected_context), total=len(selected_context), desc=\"Processing contexts\"):\n",
    "    # if i % 100 == 0:\n",
    "    #     print(i)\n",
    "\n",
    "    if random.randint(0, 1) == 0:\n",
    "        while True:\n",
    "            context_num = random.randint(0, len(unique_context) - 1)\n",
    "            not_found=True\n",
    "\n",
    "            if (context_num not in random_numbers \n",
    "                and context_num not in seen \n",
    "                and at_least_one_sensible_sentences(unique_context[context_num])):\n",
    "                \n",
    "                sentences = break_into_sentences(unique_context[context_num])\n",
    "                seen.add(context_num)\n",
    "                \n",
    "                for sent in sentences:\n",
    "                    if is_english_and_sensible(sent):\n",
    "                        negatives.append(sent)\n",
    "                        not_found=False\n",
    "                        break\n",
    "                if(not_found):\n",
    "                    for sent in sentences:\n",
    "                        if is_english(sent):\n",
    "                            negatives.append(sent)\n",
    "                            not_found=False\n",
    "                            break\n",
    "                break    \n",
    "    else:\n",
    "        present_sentence = sentance_number[str(i)]\n",
    "        sentences = break_into_sentences(context)\n",
    "        not_found=True\n",
    "        \n",
    "        for j, sent in enumerate(sentences):\n",
    "            if j != present_sentence and is_english_and_sensible(sent) and abs(j - present_sentence) > 2:\n",
    "                negatives.append(sent)\n",
    "                not_found=False\n",
    "                break\n",
    "        if(not_found):\n",
    "            for j, sent in enumerate(sentences):\n",
    "                if j != present_sentence and is_english(sent):\n",
    "                    negatives.append(sent)\n",
    "                    not_found=False\n",
    "                    break\n",
    "                \n",
    "                \n",
    "            \n",
    "\n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce103df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(negatives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8c7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "length=len(negatives) \n",
    "negatives_cleaned=[]\n",
    "\n",
    "for i in range(length):\n",
    "    neg=negatives[i]\n",
    "    sentances=break_into_sentences(neg)\n",
    "    if(len(sentances)==1):\n",
    "        negatives_cleaned.append(neg)\n",
    "\n",
    "len(negatives_cleaned)          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "FineTune_dataset=[]\n",
    "for i in range(len(selected_context)):\n",
    "    temp={}\n",
    "    temp['anchor']=selected_sentances[i]\n",
    "    temp['positive']=rephrased[i]\n",
    "    temp['negative']=negatives[i]\n",
    "    FineTune_dataset.append(temp)\n",
    "\n",
    "\n",
    "FineTune_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db71c695-94f0-424f-bcca-77444a43adf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('anchor.pkl','wb') as f:\n",
    "    pickle.dump(selected_sentances,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccc4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('positives.pkl','wb') as f:\n",
    "    pickle.dump(rephrased,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb27452",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('negatives.pkl','wb') as f:\n",
    "    pickle.dump(negatives,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e729258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('FineTuneDataset.json','w') as f:\n",
    "    json.dump(FineTune_dataset,f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f77829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(path):\n",
    "    with open(path,'rb') as f:\n",
    "        li=pickle.load(f)\n",
    "\n",
    "    filter_text=[text.strip().replace('\\n',' ') for text in li]\n",
    "    with open(path,'wb') as f:\n",
    "        pickle.dump(filter_text,f)\n",
    "\n",
    "clean_text('anchor.pkl')\n",
    "clean_text('positives.pkl')\n",
    "clean_text('negatives.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
