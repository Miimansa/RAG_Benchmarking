{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a248f108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import boto3\n",
    "#import anthropic_bedrock\n",
    "#from anthropic_bedrock import AnthropicBedrock\n",
    "from tqdm import tqdm\n",
    "import hashlib\n",
    "import argparse\n",
    "import csv\n",
    "import re\n",
    "import json\n",
    "import configparser\n",
    "import os\n",
    "import numpy as np\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02f9ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the claude qac file\n",
    "all_qac=pd.read_csv('all_qac_triplets.csv').drop_duplicates(subset=['original_ques','context','filename'])[['original_ques','context','filename']]\n",
    "#all_qac=pd.read_csv('all_qac_triplets.csv')\n",
    "all_qac.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023af73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_qac))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558f2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=set(['/'.join(name.split('/')[:4]) for name in all_qac['filename']])\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffed9b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_docs={}\n",
    "for name in all_qac['filename']:\n",
    "  temp='/'.join(name.split('/')[:4])\n",
    "  if temp in all_docs.keys():\n",
    "      all_docs[temp]+=1\n",
    "  else:\n",
    "      all_docs[temp]=1\n",
    "all_docs      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd86e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_qac['ID'] = all_qac['filename'].apply(lambda x: x.split(\"/\")[-1].replace(\".txt\",\"\"))\n",
    "all_qac.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674fd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the unique contexts\n",
    "context_id = all_qac.drop_duplicates(subset=['context','ID'])\n",
    "unique_context = context_id[\"context\"].tolist()\n",
    "unique_id = context_id['ID'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55291523",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fefce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt for generating the qac from llama\n",
    "def prompt2(alltext):\n",
    "  return f'''I'm going to give you a part of clinical trial protocol document. Then I'm going to ask you to generate question-answer pairs using the given document as context. Here is the document:\n",
    "\n",
    "<document>\n",
    "\n",
    "{alltext}\n",
    "\n",
    "</document>\n",
    "\n",
    "Genrate one or more question answer pairs based on the quote.\n",
    "\n",
    "The question should “Q”, followed by a space and it’s index such as “ 1:”.\n",
    "\n",
    "Each question should be followed by an answer, starting with \"Answer:\". Do not include or reference quoted content verbatim in the answer. Don't say \"According to Context\" when answering. .\n",
    "\n",
    "Thus, the format of your overall response should look like what's shown between the tags. Make sure to follow the formatting and spacing exactly.\n",
    "\n",
    "<example>\n",
    "\n",
    "Q1: What is the indication for the study treatment?\n",
    "\n",
    "Answer: The indication is treatment of relapsed/refractory DLBCL, PMBCL, TFL, and HGBCL after ≥2 lines of systemic therapy.\n",
    "\n",
    "Q2: What type of study is KTE-C19-101?\n",
    "\n",
    "Answer: KTE-C19-101 is an open-label, multicenter Phase 1-2 study. [4]\n",
    "\n",
    "Q2: What is being evaluated in the study?\n",
    "\n",
    "Answer: The study is evaluating safety and efficacy of axicabtagene ciloleucel.\n",
    "\n",
    "</example>\n",
    "\n",
    "Include less than or equal to 15 such question-answer pairs based on the context provided , whenever, possible. The questions-answers should be strictly from the context and follow the format provided.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db3651a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt(prompt):\n",
    "    #https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/\n",
    "    return f'''<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "You are a helpful assistant. Your carefully follow instructions.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad6716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output generation\n",
    "prompts=[]\n",
    "for text in unique_context:\n",
    "    prompt=system_prompt(prompt2(text))\n",
    "    prompts.append(prompt)\n",
    "llm = LLM(model=\"/lockbox/models/Meta-Llama-3-8B-Instruct\", gpu_memory_utilization = 0.8)\n",
    "sampling_params = SamplingParams(temperature=0.05, top_p=0.95, top_k = 40, max_tokens = 2048,)\n",
    "out=[]\n",
    "try:\n",
    "    generated_outputs = llm.generate(prompts, sampling_params)\n",
    "    for output in generated_outputs:\n",
    "        prompt = output.prompt\n",
    "        generated_text = output.outputs[0].text\n",
    "        out.append(generated_text) \n",
    "except Exception:\n",
    "            print(dir, ':', Exception)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fe310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the output to the text file\n",
    "for i in range(len(out)):\n",
    "    temp=out[i]\n",
    "    filename=f'./Test_Documents/all_context_output/out{i}.txt'\n",
    "    with open(filename,'w') as f:\n",
    "        f.write(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323ae406",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the json from the text files\n",
    "\n",
    "def createJson(dir):\n",
    "    QnAs = []\n",
    "    all_dir = [x[0] for x in os.walk(dir) if not x[0].endswith('.ipynb_checkpoints')]\n",
    "\n",
    "    for dir in tqdm(all_dir):\n",
    "        print('<directory> ', dir, ' </directory>')\n",
    "\n",
    "        # Loop through all response directories\n",
    "        for file_name in tqdm(os.listdir(dir)):\n",
    "            file_path = os.path.join(dir, file_name)\n",
    "            \n",
    "            # Skip non .txt and empty files\n",
    "            if file_name.endswith(\".txt\") and os.stat(file_path).st_size != 0:\n",
    "                index=int(re.search(r'\\d+', file_name).group())\n",
    "                #print('<file> ', file_path, ' </file>')\n",
    "\n",
    "                # Check if output_file_path is a file\n",
    "                if os.path.isfile(file_path):\n",
    "                    QnAs.append({'filename':context_id.iloc[index]['filename'],'context':unique_context[index], 'qas': []})\n",
    "\n",
    "                    with open(file_path, \"r\") as file:\n",
    "                        alltext = file.read()\n",
    "\n",
    "                        # Replace A1: with Answer: [Keep only one format]\n",
    "                        pattern = r\"\\nA\\d+: \"\n",
    "                        alltext = re.sub(pattern, \"\\nAnswer: \", alltext)\n",
    "                        p = re.compile(r'Q \\d+: (.+?)\\nAnswer: (.+?)(?=\\nQ \\d+: |$)', re.DOTALL)\n",
    "                        p2 = re.compile(r'Q\\d+: (.+?)\\nAnswer: (.+?)(?=\\nQ\\d+: |$)', re.DOTALL)\n",
    "\n",
    "                        \n",
    "                        qas = []\n",
    "\n",
    "                        for m in p.finditer(alltext):\n",
    "                            question = m.group(1).strip()\n",
    "                            answer = m.group(2).strip()\n",
    "                            qas.append({'question': question, 'answer': answer})\n",
    "                        \n",
    "                        for m in p2.finditer(alltext):\n",
    "                            question = m.group(1).strip()\n",
    "                            answer = m.group(2).strip()\n",
    "                            qas.append({'question': question, 'answer': answer})    \n",
    "\n",
    "                        QnAs[-1]['qas'] = qas\n",
    "\n",
    "    return QnAs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0405e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path='./Test_Documents/all_context_output/'\n",
    "json_data = createJson(directory_path)\n",
    "\n",
    "# Save JSON data to a file\n",
    "with open('output_all_qac.json', 'w') as f:\n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(\"JSON data saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b56f42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('output_all_qac.json', 'r') as f:\n",
    "    json_data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87abe0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to remove duplicate questions across all files\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def remove_duplicate_questions(data):\n",
    "    # Dictionary to store seen questions across all files\n",
    "    seen_questions = defaultdict(list)\n",
    "    all_files={}\n",
    "    # Iterate through each entry in the data\n",
    "    for entry in data:\n",
    "        qas = entry['qas']\n",
    "        \n",
    "        # Filter out duplicates\n",
    "        unique_questions = set()\n",
    "        unique_qas = []\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            if question not in unique_questions:\n",
    "                unique_qas.append(qa)\n",
    "                unique_questions.add(question)\n",
    "\n",
    "        # Update the entry with unique QAs\n",
    "        entry['qas'] = unique_qas\n",
    "        if(len(unique_qas)==0 and len(qas)!=0):\n",
    "            print(entry)\n",
    "        # Store each unique question with its entry\n",
    "        for qa in unique_qas:\n",
    "            seen_questions[qa['question']].append(entry)\n",
    "\n",
    "    # Randomly select one entry per unique question\n",
    "    sum=set()\n",
    "    for question, entries in seen_questions.items():\n",
    "        for entry in entries:\n",
    "          sum.add(entry['filename'])\n",
    "        \n",
    "    print(len(sum))\n",
    "    processed_data = []\n",
    "    for question, entries in seen_questions.items():\n",
    "        num=len(entries)-1\n",
    "        rand=random.randint(0,num)\n",
    "        for i in range(num+1):\n",
    "            if(i!=rand):\n",
    "                all_qac=entries[i]['qas']\n",
    "                {entries[i]['qas'].remove(qa) for qa in all_qac if qa['question']==question}\n",
    "\n",
    "            all_files[entries[i]['filename']]=entries[i]\n",
    "    for key,value in all_files.items():\n",
    "            processed_data.append(value) \n",
    "                \n",
    "    return processed_data\n",
    "\n",
    "# Example JSON data\n",
    "\n",
    "\n",
    "# Remove duplicate questions\n",
    "processed_data = remove_duplicate_questions(json_data)\n",
    "\n",
    "# Print the processed data\n",
    "#print(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "663764d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15589\n"
     ]
    }
   ],
   "source": [
    "print(len(json_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43cbbf84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14820\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e5d7292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Num_qa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CT-Documents-Hub/50-CQAGen/Pending/CART-Paper/...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Filename  Num_qa\n",
       "0  CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...       5\n",
       "1  CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...       2\n",
       "2  CT-Documents-Hub/50-CQAGen/Pending/CART-Paper/...      14\n",
       "3  CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...       7\n",
       "4  CT-Documents-Hub/50-CQAGen/Pending/NSCLC-Paper...       8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows=[]\n",
    "sum=0\n",
    "for temp in processed_data:\n",
    "    filename=temp['filename']\n",
    "    num_ques=len(temp['qas'])\n",
    "    sum=sum+num_ques\n",
    "    rows.append([filename,num_ques])\n",
    "\n",
    "df_testqa_num=pd.DataFrame(rows,columns=['Filename','Num_qa'])\n",
    "df_testqa_num.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d557ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testqa_num.to_csv(\"tesqa_count_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf5e8d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142209\n"
     ]
    }
   ],
   "source": [
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab2d49b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_all_qac_cleaned.json', 'w') as f:\n",
    "    json.dump(processed_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9205f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.to_csv('new_qac_count.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
